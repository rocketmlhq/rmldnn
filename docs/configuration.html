
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Configuration &#8212; RocketML 1.0.0 (RocketML Confidential) documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.0.0 (RocketML Confidential)',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Applications" href="applications.html" />
    <link rel="prev" title="Deep Neural Networks" href="deep_neural_networks.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="configuration">
<h1>Configuration<a class="headerlink" href="#configuration" title="Permalink to this headline">¶</a></h1>
<p>The json file must contain one single object named <code class="code docutils literal"><span class="pre">neural_network</span></code>, inside which all configuration will reside:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="o">{</span>
    <span class="s2">&quot;neural_network&quot;</span>: <span class="o">{</span>
        <span class="s2">&quot;outfile&quot;</span>: <span class="s2">&quot;log_file.txt&quot;</span>,
        <span class="s2">&quot;num_epochs&quot;</span>: <span class="m">100</span>,
        <span class="s2">&quot;data&quot;</span>: <span class="o">{</span>
            ...
        <span class="o">}</span>,
        <span class="s2">&quot;optimizer&quot;</span>: <span class="o">{</span>
            ...
        <span class="o">}</span>,
        <span class="s2">&quot;loss&quot;</span>: <span class="o">{</span>
            ...
        <span class="o">}</span>,
        <span class="s2">&quot;layers&quot;</span>: <span class="o">{</span>
            ...
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<p>The <code class="code docutils literal"><span class="pre">neural_network</span></code> object contains several sub-objects (sections) which will be discussed below, in addition to
a few basic parameters:</p>
<ul class="simple">
<li><strong>outfile</strong>: Base name for the output files produced by the run (with loss values, accuracies, etc). If not provided, no output files are created.</li>
<li><strong>num_epochs</strong>: How many total epochs to run for (default: 1).</li>
<li><strong>debug</strong>: Whether to enable debug output. The specific behavior (i.e., what debug data exactly is created) depends on the application.
For instance, if <cite>debug = true</cite>, the image segmentation application will write out images produced during inference.</li>
<li><strong>debug_interval</strong>: How often to write out debug info (in terms of number of epochs).</li>
</ul>
<div class="section" id="optimizer-section">
<h2>Optimizer section<a class="headerlink" href="#optimizer-section" title="Permalink to this headline">¶</a></h2>
<p>This section configures the optimizer for the neural network, which can be selected with the parameter <code class="code docutils literal"><span class="pre">type</span></code>.
RocketML supports the most important first-order algorithms available in PyTorch
(module <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#algorithms/">torch.optim</a>),
as well as a Hessian-based second-order optimizer.
Each optimizer type has its own set of supported hyper-parameters:</p>
<ul>
<li><p class="first">SGD:</p>
<ul class="simple">
<li><strong>learning_rate</strong>: Base learning rate (default: 0.01)</li>
<li><strong>momentum</strong>: Momentum factor (default: 0)</li>
<li><strong>weight_decay</strong>: Weight decay (L2 penalty) (default: 0)</li>
<li><strong>dampening</strong>: Dampening for momentum (default: 0)</li>
<li><strong>nesterov</strong>: Enables Nesterov momentum (default: false)</li>
</ul>
</li>
<li><p class="first">Adagrad:</p>
<ul class="simple">
<li><strong>learning_rate</strong>: Base learning rate (default: 0.01)</li>
<li><strong>lr_decay</strong>: Learning rate decay (default: 0)</li>
<li><strong>weight_decay</strong>: Weight decay (L2 penalty) (default: 0)</li>
</ul>
</li>
<li><p class="first">Adam and AdamW:</p>
<ul class="simple">
<li><strong>learning_rate</strong>: The base learning rate (default: 0.01)</li>
<li><strong>beta1</strong> and <strong>beta2</strong>: Coefficients used for computing running averages of gradient and its square (defaults: 0.9 and 0.999)</li>
<li><strong>weight_decay</strong>: weight decay (L2 penalty) (default: 0)</li>
<li><strong>eps</strong>: Term added to the denominator to improve numerical stability (default: 1e-8)</li>
<li><strong>amsgrad</strong>: Whether to use the AMSGrad variant of this algorithm (default: false)</li>
</ul>
</li>
<li><p class="first">RMSprop:</p>
<ul class="simple">
<li><strong>learning_rate</strong>: Base learning rate (default: 0.01)</li>
<li><strong>momentum</strong>: Momentum factor (default: 0)</li>
<li><strong>alpha</strong>: Smoothing constant (default: 0.99)</li>
<li><strong>eps</strong>: Term added to the denominator to improve numerical stability (default: 1e-8)</li>
<li><strong>centered</strong>: If true, compute the centered RMSProp, and normalize the gradient by its variance (default: false)</li>
<li><strong>weight_decay</strong>: Weight decay (L2 penalty) (default: 0)</li>
</ul>
</li>
<li><p class="first">LARS (see <a class="reference external" href="https://arxiv.org/pdf/1708.03888.pdf">https://arxiv.org/pdf/1708.03888.pdf</a>):</p>
<p>SGD-based first-order optimizer suitable for large-batch training.
It accepts all parameters of the SGD optimizer, plus the LARS coefficient:</p>
<ul class="simple">
<li><strong>eta</strong>: LARS’s coefficient <img class="math" src="_images/math/5635a7c34414599c2452d72430811e816b460335.png" alt="\eta"/>, or trust-ratio multiplier (default: 1e-3)</li>
</ul>
</li>
<li><p class="first">LAMB (see <a class="reference external" href="https://arxiv.org/pdf/1904.00962.pdf">https://arxiv.org/pdf/1904.00962.pdf</a>):</p>
<p>Adam-based first-order optimizer suitable for large-batch training.
It accepts all parameters of the Adam optimizer.</p>
</li>
<li><p class="first">Hessian:</p>
<ul class="simple">
<li><strong>max_iterations</strong>: Maximum number of iterations (default: 2000)</li>
<li><strong>max_func_eval</strong>: Maximum number of objective function evaluations (default: 4000)</li>
<li><strong>absolute_tolerance</strong>: Absolute tolerance (default: 1e-8)</li>
<li><strong>relative_tolerance</strong>: Relative tolerance (default: 1e-8)</li>
</ul>
</li>
</ul>
<p>Therefore, a typical example of invoking the Adagrad optimizer would look like this:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="s2">&quot;optimizer&quot;</span>: <span class="o">{</span>
    <span class="s2">&quot;type&quot;</span>: <span class="s2">&quot;Adagrad&quot;</span>,
    <span class="s2">&quot;learning_rate&quot;</span>: <span class="m">0</span>.001,
    <span class="s2">&quot;lr_decay&quot;</span>: 1e-5
<span class="o">}</span>
</pre></div>
</div>
<div class="section" id="lr-scheduler-sub-section">
<h3>LR scheduler sub-section<a class="headerlink" href="#lr-scheduler-sub-section" title="Permalink to this headline">¶</a></h3>
<p>An optional <em>learning rate scheduler</em> can be attached to the optimizer in order to automatically adjust the learning rate during training.
This can be accomplished by adding a <code class="docutils literal"><span class="pre">lr_scheduler</span></code> section under the <code class="docutils literal"><span class="pre">optimizer</span></code> section in the configuration.
For example, to engage an <em>exponential decay</em> scheduler to the Adam optimizer, one can do:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="s2">&quot;optimizer&quot;</span>: <span class="o">{</span>
    <span class="s2">&quot;type&quot;</span>: <span class="s2">&quot;Adam&quot;</span>,
    <span class="s2">&quot;learning_rate&quot;</span>: <span class="m">0</span>.01,
    <span class="s2">&quot;lr_scheduler&quot;</span>: <span class="o">{</span>
        <span class="s2">&quot;type&quot;</span>: <span class="s2">&quot;Exponential&quot;</span>,
        <span class="s2">&quot;gamma&quot;</span>: <span class="m">0</span>.2,
        <span class="s2">&quot;verbose&quot;</span>: <span class="nb">true</span>
    <span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<p>In this case, the <code class="docutils literal"><span class="pre">learning_rate</span></code> parameter will control the initial LR value, which will then be adjusted by the
scheduler at the end of each epoch.</p>
<p>The following LR schedulers are currently supported in <code class="code docutils literal"><span class="pre">rmldnn</span></code>:</p>
<ul>
<li><p class="first"><strong>Step LR</strong>: Decays the learning rate by <img class="math" src="_images/math/3666981dc77862de77b6ecfcb64aad59b425cbaf.png" alt="\gamma"/> at every <em>step_size</em> epochs.</p>
<blockquote>
<div><p><code class="code docutils literal"><span class="pre">{</span> <span class="pre">&quot;type&quot;:</span> <span class="pre">&quot;Step&quot;,</span> <span class="pre">&quot;gamma&quot;:</span> <span class="pre">0.1,</span> <span class="pre">&quot;step_size&quot;:</span> <span class="pre">2</span> <span class="pre">}</span></code></p>
</div></blockquote>
</li>
<li><p class="first"><strong>Multi-step LR</strong>: Decays the learning rate <img class="math" src="_images/math/3666981dc77862de77b6ecfcb64aad59b425cbaf.png" alt="\gamma"/> once the number of epoch reaches one of the milestones.</p>
<blockquote>
<div><p><code class="code docutils literal"><span class="pre">{</span> <span class="pre">&quot;type&quot;:</span> <span class="pre">&quot;MultiStep&quot;,</span> <span class="pre">&quot;gamma&quot;:</span> <span class="pre">0.1,</span> <span class="pre">&quot;milestones&quot;:</span> <span class="pre">[2,5,20,50]</span> <span class="pre">}</span></code></p>
</div></blockquote>
</li>
<li><p class="first"><strong>Exponential LR</strong>: Decays the learning rate by <img class="math" src="_images/math/3666981dc77862de77b6ecfcb64aad59b425cbaf.png" alt="\gamma"/> at the end of every single epoch.</p>
<blockquote>
<div><p><code class="code docutils literal"><span class="pre">{</span> <span class="pre">&quot;type&quot;:</span> <span class="pre">&quot;Exponential&quot;,</span> <span class="pre">&quot;gamma&quot;:</span> <span class="pre">0.1</span> <span class="pre">}</span></code></p>
</div></blockquote>
</li>
<li><p class="first"><strong>Warmup LR</strong>: Sets the initial learning rate to <code class="docutils literal"><span class="pre">start_factor</span> <span class="pre">*</span> <span class="pre">learning_rate</span></code>,
where <code class="docutils literal"><span class="pre">start_factor</span> <span class="pre">&lt;</span> <span class="pre">1</span></code>, then scales it up for the next <code class="docutils literal"><span class="pre">num_epochs</span></code> until it reaches <code class="docutils literal"><span class="pre">learning_rate</span></code>.</p>
<blockquote>
<div><p><code class="code docutils literal"><span class="pre">{</span> <span class="pre">&quot;type&quot;:</span> <span class="pre">&quot;Warmup&quot;,</span> <span class="pre">&quot;num_epochs&quot;:</span> <span class="pre">5,</span> <span class="pre">&quot;start_factor&quot;:</span> <span class="pre">0.2</span> <span class="pre">}</span></code></p>
</div></blockquote>
</li>
</ul>
</div>
</div>
<div class="section" id="layers-section">
<h2>Layers section<a class="headerlink" href="#layers-section" title="Permalink to this headline">¶</a></h2>
<p>This section allows for detailed specification of all layers in the neural network, as well as the connections between them.
The syntax is supposed to follow closely the one used by Keras, which allows exporting a programmatically built neural network
as a json file – see the <a class="reference external" href="https://keras.io/">Keras documentation</a>. Not all functionality exposed by Keras has been integrated into
RocketML, though, either due to being low priority, or because they would require support for different network architectures
not currently available in <code class="code docutils literal"><span class="pre">rmldnn</span></code>.</p>
<p>One can either put the network description on a separate file (e.g., <cite>model.json</cite>) and pass the file name to RocketML configuration,</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="s2">&quot;layers&quot;</span>: <span class="s2">&quot;../path/model.json&quot;</span>
</pre></div>
</div>
<p>or enter it directly as an array of json objects, one for each layer:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="s2">&quot;layers&quot;</span>: <span class="o">[</span>
    <span class="o">{</span>
        <span class="s2">&quot;class_name&quot;</span>: <span class="s2">&quot;Conv2D&quot;</span>,
        <span class="s2">&quot;config&quot;</span>: <span class="o">{</span>
            <span class="s2">&quot;name&quot;</span>: <span class="s2">&quot;layer1&quot;</span>,
            <span class="s2">&quot;trainable&quot;</span>: true,
            ...
        <span class="o">}</span>
    <span class="o">}</span>,
    <span class="o">{</span>
        <span class="s2">&quot;class_name&quot;</span>: <span class="s2">&quot;MaxPooling2D&quot;</span>,
        <span class="s2">&quot;config&quot;</span>: <span class="o">{</span>
            <span class="s2">&quot;name&quot;</span>: <span class="s2">&quot;layer2&quot;</span>,
            <span class="s2">&quot;trainable&quot;</span>: true,
            ...
        <span class="o">}</span>
    <span class="o">}</span>,
    ...
<span class="o">]</span>
</pre></div>
</div>
<p>The configuration parameters available for each layer are, of course, specific to the functionality of that particular layer.
Please refer to the Keras documentation for details. For example, a two-dimensional convolutional layer is represented in Keras
by a <code class="code docutils literal"><span class="pre">Conv2D</span></code> object, which accepts the following configuration parameters, among others:</p>
<ul class="simple">
<li><strong>filters</strong>: The number of channels of the output (i.e., number of output filters in the convolution)</li>
<li><strong>kernel_size</strong>: An integer or list of 2 integers specifying the height and width of the 2D convolution window</li>
<li><strong>strides</strong>:  An integer or list of 2 integers specifying the strides of the convolution along the height and width</li>
<li><strong>padding</strong>: An integer or list of 2 integers specifying the amount of zero-padding along the height and width.
Also accepts a string with either <cite>same</cite> or <cite>valid</cite> (Tensorflow notation)</li>
<li><strong>dilation_rate</strong>: An integer or list of 2 integers specifying the dilation rate to use for dilated convolution</li>
<li><strong>use_bias</strong>: A boolean indicating whether the layer uses a bias vector</li>
<li><strong>trainable</strong>: If set to <cite>false</cite>, the layer gets <cite>frozen</cite>, i.e., its parameters are not updated during training.
This can be applied to all trainable layers (not only <cite>Conv2d</cite>), and might be useful when loading a pre-trained model.</li>
</ul>
<p>Therefore, in order to add such a layer to the network in RocketML, the following json object could be used:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="o">{</span>
    <span class="s2">&quot;class_name&quot;</span>: <span class="s2">&quot;Conv2D&quot;</span>,
    <span class="s2">&quot;config&quot;</span>: <span class="o">{</span>
        <span class="s2">&quot;name&quot;</span>: <span class="s2">&quot;conv_layer_1&quot;</span>,
        <span class="s2">&quot;filters&quot;</span>: <span class="m">64</span>,
        <span class="s2">&quot;kernel_size&quot;</span>: <span class="o">[</span><span class="m">7</span>, <span class="m">7</span><span class="o">]</span>,
        <span class="s2">&quot;strides&quot;</span>: <span class="o">[</span><span class="m">2</span>, <span class="m">2</span><span class="o">]</span>,
        <span class="s2">&quot;padding&quot;</span>: <span class="s2">&quot;valid&quot;</span>,
        <span class="s2">&quot;dilation_rate&quot;</span>: <span class="o">[</span><span class="m">1</span>, <span class="m">1</span><span class="o">]</span>,
        <span class="s2">&quot;use_bias&quot;</span>: <span class="nb">true</span>
        <span class="s2">&quot;activation&quot;</span>: <span class="s2">&quot;ReLU&quot;</span>,
        <span class="s2">&quot;trainable&quot;</span>: <span class="nb">true</span>
    <span class="o">}</span>,
    <span class="s2">&quot;inbound_nodes&quot;</span>: <span class="o">[</span>
        <span class="o">[</span>
            <span class="o">[</span>
                <span class="s2">&quot;input_1&quot;</span>,
                <span class="m">0</span>,
                <span class="m">0</span>,
                <span class="o">{}</span>
            <span class="o">]</span>
        <span class="o">]</span>
    <span class="o">]</span>
<span class="o">}</span>
</pre></div>
</div>
<p>The parameter <code class="code docutils literal"><span class="pre">inbound_nodes</span></code> is used to indicate which layers feed into <cite>conv_layer_1</cite>. If not specified, RocketML assumes
that the output of the previous layer becomes the input of the next layer. This parameter can be a list of layers, which must all feed into a
so-called <cite>merge layer</cite>, which then combines the incoming data tensors into a single tensor (via either concatenation, addition, or subtraction).</p>
</div>
<div class="section" id="loss-section">
<h2>Loss section<a class="headerlink" href="#loss-section" title="Permalink to this headline">¶</a></h2>
<p>This section specifies which loss function to use for the neural network. The loss function computes some kind of metric that estimates
the error (loss) between the network result for a given input and its corresponding target.</p>
<p>The choice of loss function must be consistent with the network design, in particular, with the last layer in the network and its activation.
For example, the Negative Log-Likelihood (NLL) loss function expects its input to contain the log-probabilities of each class.
This can be accomplished, for example, by terminating the network with a Log-Softmax activation function.</p>
<p><code class="code docutils literal"><span class="pre">rmldnn</span></code> currently supports several types of loss functions, some of which are directly available in PyTorch, while others are
custom implementations:</p>
<ul class="simple">
<li><strong>nll</strong>: Log-Likelihood (NLL) loss function. Useful to train a classification problem with <img class="math" src="_images/math/afce44aa7c55836ca9345404c22fc7b599d2ed84.png" alt="C"/> classes. Accepts an optional
list of weights to be applied to each class.</li>
<li><strong>bce</strong>: Binary cross entropy loss function. Useful for measuring the reconstruction error in, for example, auto-encoders.</li>
<li><strong>mse</strong>: Mean squared error (squared L2 norm) loss function.</li>
<li><strong>Dice</strong>: Computes the Dice coefficient (a.k.a. F1-score) between output and target.</li>
<li><strong>Jaccard</strong>: Computes the Jaccard score (a.k.a. Intersection-over-Union, or IoU) between output and target.</li>
<li><strong>Focal</strong>: Computes the focal loss, a generalization of the cross entropy loss suitable for highly imbalanced classes.</li>
<li><strong>Lovasz</strong>: Computes an optimization of the mean IoU loss based on the convex Lovasz extension of sub-modular losses.</li>
<li><strong>Wasserstein</strong>: Used exclusively in GANs to maximize the gap between scores from real and generated samples (<code class="code docutils literal"><span class="pre">--app=gan</span></code>)</li>
<li><strong>YOLOv3</strong>: Used exclusively for object detection (<code class="code docutils literal"><span class="pre">--app=obj</span></code>)</li>
<li><strong>Burgers_pde</strong>: Loss function encoded as an invariant (PDE + boundary condition) of the Burgers’ 1+1-dimensional
partial differential equation (<code class="code docutils literal"><span class="pre">--app=pde</span></code>).</li>
<li><strong>Poisson2D_pde</strong>: Invariant loss function for the 2D Poisson PDE (<code class="code docutils literal"><span class="pre">--app=pde</span></code>).</li>
<li><strong>Poisson3D_pde</strong>: Invariant loss function for the 3D Poisson PDE (<code class="code docutils literal"><span class="pre">--app=pde</span></code>).</li>
</ul>
<p>A typical way to engage, for example, the NLL loss function would be:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="s2">&quot;loss&quot;</span>: <span class="o">{</span>
    <span class="s2">&quot;function&quot;</span>: <span class="s2">&quot;NLL&quot;</span>,
    <span class="s2">&quot;weight&quot;</span>: <span class="o">[</span><span class="m">0</span>.3, <span class="m">0</span>.4, <span class="m">0</span>.5, <span class="m">0</span>.6, <span class="m">0</span>.7, <span class="m">0</span>.8<span class="o">]</span>
<span class="o">}</span>
</pre></div>
</div>
</div>
<div class="section" id="data-section">
<h2>Data section<a class="headerlink" href="#data-section" title="Permalink to this headline">¶</a></h2>
<p>This is the section where the types of training and test data are configured, in particular, what specific data loader will be used
to feed data into the neural network, as well as how that data will be split into mini-batches,
how many samples will be used for training and evaluation, etc.</p>
<p>The following data types are currently supported in RocketML:</p>
<ul class="simple">
<li><strong>mnist</strong>: Loads data from the MNIST handwritten digits database in binary format.</li>
<li><strong>images</strong>: Loads image files which can be used for classification (images and labels), segmentation (images and masks), autoencoders, etc.</li>
<li><strong>labels</strong>: Automatically determines class labels based on the names of the directories where sample input files are located (for classification only).</li>
<li><strong>numpy</strong>: Loads data from NumPy arrays in either <code class="code docutils literal"><span class="pre">.npy</span></code> format (one sample per file) or <code class="code docutils literal"><span class="pre">.npz</span></code> format (multiple samples per file).
Also supports the data slicing capability described below.</li>
<li><strong>pde</strong>: Generates initial conditions to be used with a DNN-based partial differential equation solver.</li>
</ul>
<p>The following parameters apply to all data loader types, and are critical to configuring the run:</p>
<ul class="simple">
<li><strong>input_type</strong>: Input data type.</li>
<li><strong>target_type</strong>: Target data type.</li>
<li><strong>type</strong>: If input and target types are the same, this parameter can be used for simplicity.</li>
<li><strong>input_path</strong>: Path to directory with training input samples. If not defined, the training step is skipped.</li>
<li><strong>target_path</strong>: Path to directory with training target samples. Required only for certain applications (e.g., segmentation)</li>
<li><strong>test_input_path</strong>: Path to directory with test (evaluation) input samples. If not defined, the evaluation step is skipped.</li>
<li><strong>test_target_path</strong>: Path to directory with test target samples. If omitted, inference runs without targets (loss is not computed).</li>
<li><strong>batch_size</strong>: Number of training samples per mini-batch (default is 64).</li>
<li><strong>test_batch_size</strong>: Number of test (evaluation) samples per mini-batch (default is 64).</li>
<li><strong>preload</strong>: Whether samples will be read up-front from disk and loaded from memory during training/eval (default is <em>false</em>).</li>
<li><strong>target_is_mask</strong>: If set to <em>true</em>, target samples are handled as discrete (integer) data, e.g., operations like
rotation and resize will apply a nearest-neighbor interpolation scheme (default is <em>false</em>).</li>
<li><strong>transforms</strong>: Data transform operations that can be applied to the samples – see details below.</li>
</ul>
<p>This section also supports parameters that are specific to the type of data being loaded. For example, <cite>grayscale</cite> is a parameter that
applies to image data only, but not to numpy arrays. More details on how to configure each type of data loader will be shown in
the applications section.</p>
<div class="section" id="slicers-sub-section">
<h3>Slicers sub-section<a class="headerlink" href="#slicers-sub-section" title="Permalink to this headline">¶</a></h3>
<p>The <strong>numpy</strong> data loader supports extracting the input samples from a single large numpy array by chopping it off into smaller
blocks of configurable sizes. The samples obtained can have equal or lower dimensionality as the original data, as long as the neural
network can handle their shapes. For example, if the input numpy array is a 3D block of shape <img class="math" src="_images/math/32c738275ea0864a0729ff5ae9d9f8a1c96ec3f6.png" alt="(H,W,D)"/>,
one could chop it into smaller blocks of shape <img class="math" src="_images/math/a70680e082d0253830854d660dcfc36f206fef05.png" alt="(h,w,d"/>), where <img class="math" src="_images/math/ba91f86e467a31f7bff59f60e84560ec216034d5.png" alt="h \le H"/>, <img class="math" src="_images/math/ffc0d2af687a563feba436a54118a8db24b377c1.png" alt="w \le W"/> and <img class="math" src="_images/math/87a92a98b4d03a624fe14157b41596719aa0f965.png" alt="d \le D"/>,
or slice it into 2D tiles along the <img class="math" src="_images/math/fa6e138b022ee026f2511136dc7ae1aa592471be.png" alt="xy"/>-plane with shape <img class="math" src="_images/math/74d6c08677b4a17dd8df8c1776409779d868e60d.png" alt="(h,w)"/>,
or even extract 1D lines of length <img class="math" src="_images/math/5267cecca6d578d190e686ba72fe9c7c138e249f.png" alt="w &lt; W"/> along the <img class="math" src="_images/math/276f7e256cbddeb81eee42e1efc348f3cb4ab5f8.png" alt="y"/>-axis.
Multiple slice sets can be defined, each with its own slice size and orientation (the dimensionality of slices across all sets
must be the same, though, since the neural network is common to all). The configuration below shows an example of how to extract
2D samples from a 3D input array using 2 slice sets:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="s2">&quot;data&quot;</span>: <span class="o">{</span>
    ...
    <span class="s2">&quot;slicers&quot;</span>: <span class="o">[</span>
        <span class="o">{</span>
            <span class="s2">&quot;name&quot;</span>:               <span class="s2">&quot;yz-slices&quot;</span>,
            <span class="s2">&quot;sizes&quot;</span>:              <span class="o">[</span><span class="m">1</span>, <span class="m">131</span>, <span class="m">1001</span><span class="o">]</span>,
            <span class="s2">&quot;padded_sizes&quot;</span>:       <span class="o">[</span><span class="m">1</span>, <span class="m">144</span>, <span class="m">1008</span><span class="o">]</span>,
            <span class="s2">&quot;discard_remainders&quot;</span>: false,
            <span class="s2">&quot;transpose&quot;</span>:          <span class="nb">false</span>
        <span class="o">}</span>,
        <span class="o">{</span>
            <span class="s2">&quot;name&quot;</span>:               <span class="s2">&quot;xz-slices&quot;</span>,
            <span class="s2">&quot;sizes&quot;</span>:              <span class="o">[</span><span class="m">540</span>, <span class="m">1</span>, <span class="m">1001</span><span class="o">]</span>,
            <span class="s2">&quot;padded_sizes&quot;</span>:       <span class="o">[</span><span class="m">560</span>, <span class="m">1</span>, <span class="m">1008</span><span class="o">]</span>,
            <span class="s2">&quot;discard_remainders&quot;</span>: false,
            <span class="s2">&quot;transpose&quot;</span>:          <span class="nb">true</span>
        <span class="o">}</span>
    <span class="o">]</span>
<span class="o">}</span>
</pre></div>
</div>
<p>The following options can be set:</p>
<ul class="simple">
<li><strong>name</strong>: Slice set name (optional)</li>
<li><strong>sizes</strong>: Slice sizes (required). Expects N elements for N-dimensional input data. Setting an element to 1 flattens the slice along that dimension,
reducing the dimensionality of the input samples into the network.</li>
<li><strong>padding</strong>: Symmetric padding to be added along each dimension (defaults to zero). If <img class="math" src="_images/math/acea77da57466a571cff27ff46b7f596da02a3be.png" alt="\textrm{sizes=} [h,w,d]"/> and
<img class="math" src="_images/math/cd250daa1e1893829bd64c40c4be1ce2950880e2.png" alt="\textrm{padding=}[p_x, p_y, p_z]"/>, then slices will have shape <img class="math" src="_images/math/baaa4f4c7058e98c9b6d29c7054bcb8b04bc39de.png" alt="(h + 2 p_x, w + 2 p_y, d + 2 p_z)"/>.
Cannot be specified together with <cite>padded_sizes</cite>.</li>
<li><strong>padded_sizes</strong>: Total slice size after padding (defaults to <cite>sizes</cite>). Useful in case the desired padding is asymmetric.
Cannot be specified together with <cite>padding</cite>.</li>
<li><strong>strides</strong>: Displacements used when slicing in each direction (defaults to <cite>sizes</cite>). If smaller than <cite>sizes</cite>, then slices will overlap.</li>
<li><strong>discard_remainders</strong>: Whether to discard regions of the input data which are left over after slicing (default is <cite>false</cite>, i.e.,
leftovers are padded up to <cite>sizes</cite> and added to the sample list).</li>
<li><strong>transpose</strong>: Whether to transpose each slice before and after network traversal. Only valid for 2D slices (default is <cite>false</cite>).</li>
</ul>
<p>The inference process, including the addition and removal of padding (as well as optional slice transposition), is
depicted in the figure below:</p>
<a class="reference internal image-reference" href="_images/slicer_padding.png"><img alt="slicer_padding.png" src="_images/slicer_padding.png" style="width: 600px;" /></a>
<p><strong>HDF5 output writing</strong></p>
<p>The predictions obtained by running inferences on the slices can be assembled back into a multidimensional array and saved to disk
as an HDF5 file. Each slice set will result in one dataset in the HDF5 data-structure.
In order to enable HDF5 writing, set the following:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="s2">&quot;data&quot;</span>: <span class="o">{</span>
    ...
    <span class="s2">&quot;hdf5_outfile&quot;</span>: <span class="s2">&quot;prediction.h5&quot;</span>
    ...
<span class="o">}</span>
</pre></div>
</div>
<p>The process of writing data into the HDF5 file is performed in parallel (in case of multi-process execution)
and asynchronously, i.e., it happens concurrently with inference in order to maximize throughput.
The entire infrastructure for data slicing, inferencing and assembling is depicted in the figure below.</p>
<a class="reference internal image-reference" href="_images/slicer_flow.png"><img alt="slicer_flow.png" src="_images/slicer_flow.png" style="width: 600px;" /></a>
<p><strong>Restrictions:</strong></p>
<ul class="simple">
<li>The input numpy array must have no channel dimension (i.e., the data must be single-channel with only spatial dimensions).</li>
<li>The shape of the output tensor produced by the network must be equal to the input shape plus an extra channel dimension.</li>
<li>Only 2D slices can be transposed.</li>
</ul>
</div>
<div class="section" id="transforms-sub-section">
<h3>Transforms sub-section<a class="headerlink" href="#transforms-sub-section" title="Permalink to this headline">¶</a></h3>
<p>The <strong>image</strong> and <strong>numpy</strong> data loaders support operations that can be applied to individual 2D samples during training.
Notice that:</p>
<blockquote>
<div><ul class="simple">
<li>Operations which are stochastic in nature (e.g., random rotation or random zoom) result in different samples being produced
at different epochs, thus providing a mechanism for data augmentation that should enhance training convergence.</li>
<li>Operations which require resizing (e.g., rotation, zooming, resize) apply a linear interpolation scheme by default.
If the targets contain discrete data (e.g., masks with segmentation labels), one should set <code class="docutils literal"><span class="pre">target_is_mask</span></code> to <em>true</em>
(see <strong>Data</strong> section), so that a nearest-neighbor interpolation scheme is used for them instead.</li>
</ul>
</div></blockquote>
<p>The following transformations are supported:</p>
<ul>
<li><p class="first"><strong>resize</strong>: Resizes the sample to a given size using bilinear interpolation.</p>
<blockquote>
<div><p>Usage: <code class="code docutils literal"><span class="pre">resize:</span> <span class="pre">[Sx,</span> <span class="pre">Sy]</span></code>, where <img class="math" src="_images/math/72b1a0f7e9601d4ee4cf3afbc9ffbd7a9faafc0a.png" alt="S_x \times S_y"/> is the desired sample size.</p>
</div></blockquote>
</li>
<li><p class="first"><strong>center_crop</strong>: Crops the sample at the center to a given output size.</p>
<blockquote>
<div><p>Usage: <code class="code docutils literal"><span class="pre">center_crop:</span> <span class="pre">[Sx,</span> <span class="pre">Sy]</span></code>, where <img class="math" src="_images/math/72b1a0f7e9601d4ee4cf3afbc9ffbd7a9faafc0a.png" alt="S_x \times S_y"/> is the output size.</p>
</div></blockquote>
</li>
<li><p class="first"><strong>jitter_crop</strong>: Crops the sample in each direction <img class="math" src="_images/math/df0deb143e5ac127f00bd248ee8001ecae572adc.png" alt="i"/> by <img class="math" src="_images/math/1bff272fa169564c0d00fb3da08b44b4fe567ee5.png" alt="c \times S_i / 2"/>,
where <img class="math" src="_images/math/ae12a24f88803b5895632e4848d87d46483c492c.png" alt="c"/> is a random variable uniformly sampled from <img class="math" src="_images/math/40a5d1d435d43477516f7cb6ff8b2f784ce61651.png" alt="c \in [0, C_\textrm{max})"/>.</p>
<blockquote>
<div><p>Usage: <code class="code docutils literal"><span class="pre">jitter_crop:</span> <span class="pre">Cmax</span></code></p>
</div></blockquote>
</li>
<li><p class="first"><strong>random_horizontal_flip</strong>: Randomly flips the sample horizontally with a given probability <img class="math" src="_images/math/27d463da4622be5b3ef1d4176ced7d7a323c6425.png" alt="p"/>.</p>
<blockquote>
<div><p>Usage: <code class="code docutils literal"><span class="pre">random_horizontal_flip:</span> <span class="pre">p</span></code></p>
</div></blockquote>
</li>
<li><p class="first"><strong>random_vertical_flip</strong>: Randomly flips the sample horizontally with a given probability <img class="math" src="_images/math/27d463da4622be5b3ef1d4176ced7d7a323c6425.png" alt="p"/>.</p>
<blockquote>
<div><p>Usage: <code class="code docutils literal"><span class="pre">random_vertical_flip:</span> <span class="pre">p</span></code></p>
</div></blockquote>
</li>
<li><p class="first"><strong>random_zoom</strong>: Randomly zooms in by <img class="math" src="_images/math/1bff272fa169564c0d00fb3da08b44b4fe567ee5.png" alt="c \times S_i / 2"/> in each direction <img class="math" src="_images/math/df0deb143e5ac127f00bd248ee8001ecae572adc.png" alt="i"/>, where
<img class="math" src="_images/math/ae12a24f88803b5895632e4848d87d46483c492c.png" alt="c"/> is a random variable uniformly sampled from <img class="math" src="_images/math/40a5d1d435d43477516f7cb6ff8b2f784ce61651.png" alt="c \in [0, C_\textrm{max})"/>.</p>
<blockquote>
<div><p>Usage: <code class="code docutils literal"><span class="pre">random_zoom:</span> <span class="pre">Cmax</span></code></p>
</div></blockquote>
</li>
<li><p class="first"><strong>rotate</strong>: Rotates the sample clockwise by a given fixed angle.</p>
<blockquote>
<div><p>Usage: <code class="code docutils literal"><span class="pre">rotate:</span> <span class="pre">phi</span></code>, where <img class="math" src="_images/math/c2f31c22645274c375eff7920cfdfdc18d60341f.png" alt="\phi"/> is the rotation angle.</p>
</div></blockquote>
</li>
<li><p class="first"><strong>random_rotate</strong>: Rotates the sample by a random angle sampled uniformly between <img class="math" src="_images/math/cf91d4374d6c1bcb3200d667c9804732b48e1ae7.png" alt="-\alpha"/> and <img class="math" src="_images/math/a7679ce57e090edbd51e1ad9e50cfe707937fd33.png" alt="+\alpha"/>.</p>
<blockquote>
<div><p>Usage: <code class="code docutils literal"><span class="pre">random_rotate:</span> <span class="pre">alpha</span></code></p>
</div></blockquote>
</li>
<li><p class="first"><strong>convert_color</strong>: Converts the image to a different color scheme (given as an openCV <a class="reference external" href="https://vovkos.github.io/doxyrest-showcase/opencv/sphinx_rtd_theme/enum_cv_ColorConversionCodes.html">color conversion code</a>).</p>
<blockquote>
<div><p>Usage: <code class="code docutils literal"><span class="pre">convert_color:</span> <span class="pre">code</span></code></p>
</div></blockquote>
</li>
</ul>
<ul>
<li><p class="first"><strong>normalize</strong>: Normalizes the resulting tensor using a given mean <img class="math" src="_images/math/877d234f4cec6974ce218fc2e975a486a7972dfd.png" alt="\alpha"/> and
standard deviation <img class="math" src="_images/math/011e5790a6c33043ceadca81d9657dde6c61d769.png" alt="\sigma"/>, that is, <img class="math" src="_images/math/c91202c50910f66fae08009c9a2a08a849c0b879.png" alt="x' = (x - \alpha) / \sigma"/>.</p>
<blockquote>
<div><p>Usage: <code class="code docutils literal"><span class="pre">normalize:</span> <span class="pre">{&quot;mean&quot;:</span> <span class="pre">alpha,</span> <span class="pre">&quot;std&quot;:</span> <span class="pre">sigma}</span></code></p>
</div></blockquote>
</li>
</ul>
<p>Below is an example of how to use some of the above transforms.
Operations are applied in the same order as they are listed.
For that reason, if <code class="code docutils literal"><span class="pre">resize</span></code> is present, it should usually be the last operation applied,
so that all samples going into the neural network have the same size.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="s2">&quot;data&quot;</span>: <span class="o">{</span>
    ...
    <span class="s2">&quot;transforms&quot;</span>: <span class="o">[</span>
        <span class="o">{</span> <span class="s2">&quot;normalize&quot;</span>: <span class="o">{</span> <span class="s2">&quot;mean&quot;</span>: <span class="m">0</span>.5, <span class="s2">&quot;std&quot;</span>: <span class="m">0</span>.5 <span class="o">}</span> <span class="o">}</span>,
        <span class="o">{</span> <span class="s2">&quot;convert_color&quot;</span>: <span class="s2">&quot;BGR2RGB&quot;</span> <span class="o">}</span>,
        <span class="o">{</span> <span class="s2">&quot;random_horizontal_flip&quot;</span>: <span class="m">0</span>.5 <span class="o">}</span>,
        <span class="o">{</span> <span class="s2">&quot;jitter_crop&quot;</span>: <span class="m">0</span>.1 <span class="o">}</span>,
        <span class="o">{</span> <span class="s2">&quot;random_rotate&quot;</span>: <span class="m">20</span> <span class="o">}</span>,
        <span class="o">{</span> <span class="s2">&quot;resize&quot;</span>: <span class="o">[</span><span class="m">416</span>, <span class="m">416</span><span class="o">]</span> <span class="o">}</span>
    <span class="o">]</span>
<span class="o">}</span>
</pre></div>
</div>
<p>The operations listed under <code class="code docutils literal"><span class="pre">transforms</span></code> will apply to both input and target samples. In order to specify different
operations for inputs and targets, the settings <code class="code docutils literal"><span class="pre">input_transforms</span></code> and <code class="code docutils literal"><span class="pre">target_transforms</span></code> should
be used. For example, if one needs to resize inputs to a different size as the targets, one could do:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="s2">&quot;data&quot;</span>: <span class="o">{</span>
    ...
    <span class="s2">&quot;input_transforms&quot;</span>: <span class="o">[</span>
        <span class="o">{</span> <span class="s2">&quot;resize&quot;</span>: <span class="o">[</span><span class="m">128</span>, <span class="m">128</span><span class="o">]</span> <span class="o">}</span>
    <span class="o">]</span>,
    <span class="s2">&quot;target_transforms&quot;</span>: <span class="o">[</span>
        <span class="o">{</span> <span class="s2">&quot;resize&quot;</span>: <span class="o">[</span><span class="m">16</span>, <span class="m">16</span><span class="o">]</span> <span class="o">}</span>
    <span class="o">]</span>
<span class="o">}</span>
</pre></div>
</div>
<p><strong>Special-purpose transforms:</strong></p>
<ul class="simple">
<li><strong>random_patches</strong>: Extracts random square patches from the input samples,
and makes target samples from those patches. This enables unsupervised training of context encoder
networks that learn visual features via <a class="reference external" href="https://arxiv.org/pdf/1604.07379.pdf">inpainting</a>.</li>
</ul>
<p>This transform can be configured with the <cite>number</cite> of random patches and their linear <cite>size</cite>, as for example:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="s2">&quot;transforms&quot;</span>: <span class="o">[</span>
    <span class="o">{</span> <span class="s2">&quot;random_patches&quot;</span>: <span class="o">{</span> <span class="s2">&quot;number&quot;</span>: <span class="m">100</span>, <span class="s2">&quot;size&quot;</span>: <span class="m">10</span> <span class="o">}</span> <span class="o">}</span>
<span class="o">]</span>
</pre></div>
</div>
<p>In this case, pairs or input and target samples with 100 patches of size 10x10 are generated during training,
like this one:</p>
<a class="reference internal image-reference" href="_images/random_patches.png"><img alt="random_patches.png" src="_images/random_patches.png" style="width: 600px;" /></a>
</div>
</div>
<div class="section" id="checkpoints-section">
<h2>Checkpoints section<a class="headerlink" href="#checkpoints-section" title="Permalink to this headline">¶</a></h2>
<p>In order to save model checkpoints out to disk during training, one must add the <cite>checkpoints</cite> object to the <cite>json</cite> config file.
This section can also be used to load the model from file before running training. Accepted model file formats are
<code class="code docutils literal"><span class="pre">.pt</span></code> (from libtorch) and <code class="code docutils literal"><span class="pre">.h5</span></code> (HDF5 from Keras/TF).</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="s2">&quot;checkpoints&quot;</span>: <span class="o">{</span>
    <span class="s2">&quot;save&quot;</span>: <span class="s2">&quot;./checkpoint_dir/&quot;</span>
    <span class="s2">&quot;interval&quot;</span>: <span class="m">10</span>,
    <span class="s2">&quot;load&quot;</span>: <span class="s2">&quot;./model_checkpoint_100.pt&quot;</span>
<span class="o">}</span>
</pre></div>
</div>
<ul class="simple">
<li><strong>save</strong>: The directory to save model checkpoint files into.</li>
<li><strong>interval</strong>: When set to <img class="math" src="_images/math/f4170ed8938b79490d8923857962695514a8e4cb.png" alt="N"/>, will save model checkpoints at every <img class="math" src="_images/math/f4170ed8938b79490d8923857962695514a8e4cb.png" alt="N"/> epochs (defaults to 1).</li>
<li><strong>load</strong>: A previously created checkpoint file to load the model from.</li>
</ul>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Configuration</a><ul>
<li><a class="reference internal" href="#optimizer-section">Optimizer section</a><ul>
<li><a class="reference internal" href="#lr-scheduler-sub-section">LR scheduler sub-section</a></li>
</ul>
</li>
<li><a class="reference internal" href="#layers-section">Layers section</a></li>
<li><a class="reference internal" href="#loss-section">Loss section</a></li>
<li><a class="reference internal" href="#data-section">Data section</a><ul>
<li><a class="reference internal" href="#slicers-sub-section">Slicers sub-section</a></li>
<li><a class="reference internal" href="#transforms-sub-section">Transforms sub-section</a></li>
</ul>
</li>
<li><a class="reference internal" href="#checkpoints-section">Checkpoints section</a></li>
</ul>
</li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="deep_neural_networks.html" title="previous chapter">Deep Neural Networks</a></li>
      <li>Next: <a href="applications.html" title="next chapter">Applications</a></li>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/configuration.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022, RocketML.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.7</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.8</a>
      
      |
      <a href="_sources/configuration.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>